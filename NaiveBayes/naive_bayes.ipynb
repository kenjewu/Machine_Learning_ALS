{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯的实现并用于垃圾邮寄分类测试\n",
    "朴素贝叶斯是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布，然后基于此模型，对给定的输入x,利用贝叶斯定理求出后验概率最大的输出y。\n",
    "朴素贝叶斯法实际学习到生成数据的机制，即数据特征和类别的联合概率分布 P(X,Y)，所以属于生成模型。\n",
    "\n",
    "***\n",
    "## 贝叶斯公式与朴素贝叶斯模型\n",
    "$$ P(Y|X) = \\frac{P(Y)P(X|Y)}{P(X)} $$\n",
    "\n",
    "通过贝叶斯公式可以看出，朴素贝叶斯法分类时，对给定的输入x,通过学习到的模型九三后验概率分布$ P(Y=c_k|X=x) $, 将后验概率最大的类作为x的类输出。\n",
    "\n",
    "后验概率计算根据贝叶斯公式可得：\n",
    "\n",
    "$$ P(Y=c_k|X=x)=\\frac{P(Y=c_k)P(X=x|Y=c_k)}{\\sum_kP(Y=c_k)P(X=x|Y=c_k)} $$\n",
    "\n",
    "因为对于每个样本 p(x)的总是相同的，所以在不同类别的后验概率的计算中分母是相同的，可以省略掉。这样贝叶斯分类器可以表示为：\n",
    "\n",
    "$$ y=argmax_{c_k}P(Y=c_k)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_k) $$ 其中 j 代表X特征个数，k代表类别的个数\n",
    "\n",
    "***\n",
    "## 后验概率最大化的含义\n",
    "在朴素贝叶斯中，将实例分到后验概率最大的类中，这等价于期望风险最小化。\n",
    "\n",
    "***\n",
    "## 朴素贝叶斯法的参数估计\n",
    "* #### 极大似然估计法\n",
    "先验概率的极大似然估计为： $ P(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)}{N}, k=1,2,...,K $ 其中 I 是boolean函数\n",
    "\n",
    "    设第 j 个特征 $ x^{(j)} $ 可能取值的集合为 {$ a_{j1}, a_{j2},...,a_{js_j} $}，则条件概率的极大似然估计为\n",
    "    \n",
    "     $$ P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^NI(y_i=c_k)} $$\n",
    "    其中 j=1,2,...,n;  l=1,2,...,Sj;  k=1,2,...,K\n",
    "    \n",
    "* #### 贝叶斯估计法\n",
    "用极大似然估计可能会出现某个所有估计的条件概率为0，这时会导致后延概率的计算结果为0，是分类不准确。为了解决这一问题，可以采用贝叶斯估计。\n",
    "\n",
    "    给定 $ \\lambda>0 $，则先验概率的贝叶斯估计为：\n",
    "$$ P_{\\lambda}(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)+\\lambda}{N+K\\lambda} $$\n",
    "\n",
    "    条件概率的贝叶斯估计为：\n",
    "    $$ P_{\\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\\lambda}{\\sum_{i=1}^NI(y_i=c_k)+S_j\\lambda} $$\n",
    "    \n",
    "    其中 Sj 为第j个特征的取值个数。可以看出当 $ \\lambda=0 $ 时就变成了极大似然估计，特殊的当 $ \\lambda=1 $,就是拉普拉斯平滑，这样就解决了在极大似然估计中概率为0的问题。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 导入相关包\n",
    "这里导入 sklearn中的datasets包用于下载fetch_20newsgroups 数据作为本次模型的训练和测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import time\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = fetch_20newsgroups(data_home=\"./data/\", subset='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 构建一些辅助函数用于文本处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理文本中的符号\n",
    "def process_text(data):\n",
    "    processed_data = []\n",
    "    for example in data:\n",
    "        example = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，\\n。？、~@#￥%……&*（）]+\", \" \", example)\n",
    "        example = re.sub(r\"\\W\", \" \", example)\n",
    "        processed_data.append(example.lower().split())\n",
    "    return processed_data\n",
    "\n",
    "# 生成词典\n",
    "def generate_vocab(data_x):\n",
    "    \"\"\"生成词表\"\"\"\n",
    "    vocabs = set([])\n",
    "    for example in data_x:\n",
    "        vocabs = vocabs | set(example)\n",
    "    return list(vocabs)\n",
    "\n",
    "# 文本转换为向量\n",
    "def convert_data_to_vec(data_x, vocabs):\n",
    "    \"\"\"将文本数据集转换为向量，得到数据集矩阵，矩阵高为文本个数，宽为词汇表大小\"\"\"\n",
    "    data_vec = np.zeros((len(data_x), len(vocabs)))\n",
    "    for row, example in enumerate(data_x):\n",
    "        for column, word in enumerate(example):\n",
    "            if word in vocabs:\n",
    "                data_vec[row][column] = 1\n",
    "    return data_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 划分数据集\n",
    "使用 train_test_split()函数划分训练集和测试集\n",
    "\n",
    "并对训练和测试数据集的文本进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(news.data, news.target, test_size=0.25, random_state=33)\n",
    "\n",
    "((len(X_train), len(Y_train)), (len(X_test), len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理文本\n",
    "X_train = process_text(X_train)\n",
    "X_test = process_text(X_test)\n",
    "\n",
    "vocabs = generate_vocab(X_train[0:2000])\n",
    "\n",
    "X_train_sub = convert_data_to_vec(X_train[0:2000], vocabs)\n",
    "X_test_sub = convert_data_to_vec(X_test[0:20], vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_sub = Y_train[0:2000]\n",
    "Y_test_sub = Y_test[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建朴素贝叶斯模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    def __init__(self, lambd=1.0):\n",
    "        self.lambd = lambd\n",
    "        self.prior_prob = None\n",
    "        self.conditional_prob = None\n",
    "    \n",
    "    def fit(self, data_x, data_y):\n",
    "        \"\"\"训练，为了计算先验概率与条件概率\"\"\"\n",
    "        start_time = time.time()      \n",
    "        \n",
    "        # 计算先验概率\n",
    "        print(\"开始计算先验概率...\")\n",
    "        cate_num_k = len(set(data_y))\n",
    "        self.prior_prob = {}\n",
    "        for cate in set(data_y):\n",
    "            # 采用贝叶斯估计\n",
    "            self.prior_prob[cate] = (data_y.tolist().count(cate) + self.lambd) / (len(data_y) + cate_num_k * self.lambd)\n",
    "        \n",
    "        # 对训练集的每个特征的取值进行统计\n",
    "        every_feature_count = []\n",
    "        for feature_idx in range(data_x.shape[1]):\n",
    "            feature_value = data_x[:, feature_idx]\n",
    "            feature_diff_value_count = collections.Counter(feature_value)\n",
    "            every_feature_count.append(feature_diff_value_count)\n",
    "        \n",
    "        # 将数据按照类别不同划分为多个组\n",
    "        group_data = {} \n",
    "        for cate in set(data_y):      \n",
    "            sub_data_x = []\n",
    "            for idx, example_label in enumerate(data_y):\n",
    "                if example_label == cate:\n",
    "                    sub_data_x.append(data_x[idx])\n",
    "            group_data[cate] = np.asarray(sub_data_x)\n",
    "        \n",
    "        # 计算每个类别的特征的条件概率\n",
    "        print(\"开始计算条件概率...\")\n",
    "        # 所有类别下所有特征的不同取值的条件概率。\n",
    "        self.conditional_prob = {}\n",
    "        for cate in set(data_y):\n",
    "            cate_data = group_data[cate]  \n",
    "            # 某类子数据集的样本个数\n",
    "            num_cate = cate_data.shape[0]\n",
    "            \n",
    "            every_feature_cond_prob = []    \n",
    "            for idx in range(cate_data.shape[1]):\n",
    "                feature_count = every_feature_count[idx]\n",
    "                cate_feature_value = cate_data[:, idx]\n",
    "                sj = len(feature_count)\n",
    "                \n",
    "                feature_cond_prob = {}\n",
    "                for value in feature_count.keys():\n",
    "                    ajl_count = cate_feature_value.tolist().count(value)\n",
    "                    ajl_on_cate_prob = (ajl_count + self.lambd) / (num_cate + sj * self.lambd)                    \n",
    "                    feature_cond_prob[value] = ajl_on_cate_prob\n",
    "                    \n",
    "                every_feature_cond_prob.append(feature_cond_prob)\n",
    "                \n",
    "            self.conditional_prob[cate] = every_feature_cond_prob\n",
    "            \n",
    "        stop_time = time.time()\n",
    "        print(\"训练结束，耗时：{0} 秒\".format(str(stop_time-start_time)))\n",
    "        \n",
    "        return self.prior_prob, self.conditional_prob\n",
    "        \n",
    "    \n",
    "    def predict(self, data_test):\n",
    "        \"\"\"预测，在新的数据集上\"\"\"\n",
    "        \n",
    "        if self.prior_prob is None or self.conditional_prob is None:\n",
    "            raise NameError(\"模型未训练，没有可用的参数\")\n",
    "        \n",
    "        # 测试集在每个类别上的后验概率\n",
    "        test_cate_prob = np.zeros((data_test.shape[0], len(self.prior_prob)))\n",
    "        \n",
    "        cate_idx = 0\n",
    "        # 创建一个类别名称的列表，用于之后对结果的索引\n",
    "        cates_name = []\n",
    "        # 计算测试集每个样本在每个类别上的后验概率\n",
    "        for cate in self.prior_prob.keys():\n",
    "            cate_prior_prob = self.prior_prob[cate]\n",
    "            every_feature_cond_prob = self.conditional_prob[cate]   # 是个列表\n",
    "            \n",
    "            # 所有样本的概率\n",
    "            cate_test_data_cond_prob = []\n",
    "            \n",
    "            # 对每个样本进行计算\n",
    "            for example in data_test:\n",
    "                example_feature_prob = []\n",
    "                for idx, feature_value in enumerate(example.tolist()):\n",
    "                    feature_cond_prob = every_feature_cond_prob[idx]\n",
    "                    if feature_value in feature_cond_prob.keys():                 \n",
    "                        example_feature_prob.append(feature_cond_prob[feature_value])\n",
    "                    else:\n",
    "                        example_feature_prob.append(1.0)\n",
    "                cate_test_data_cond_prob.append(example_feature_prob)\n",
    "            \n",
    "            # 求所有样本在 cate 类上的对数联合概率              \n",
    "            log_cate_union_prob = np.sum(np.log(np.asarray(cate_test_data_cond_prob)), axis=1) + np.log(cate_prior_prob)\n",
    "            test_cate_prob[:, cate_idx] = log_cate_union_prob\n",
    "            cates_name.append(cate)        \n",
    "            cate_idx += 1\n",
    "            \n",
    "        # 取后验概率最大的索引\n",
    "        argmax_idx = np.argmax(test_cate_prob, axis=1)\n",
    "        # 索引出类别名称\n",
    "        test_cate_result = [cates_name[idx] for idx in argmax_idx]\n",
    "        \n",
    "        return test_cate_result\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化一个 朴素贝叶斯分类器\n",
    "NBClassifier = NaiveBayesClassifier(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_prob, conditional_prob = NBClassifier.fit(X_train_sub, Y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_sub_predict = NBClassifier.predict(X_test_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
